{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler for the scraping all the product listings from 'https://www.partsgeek.com/'. This notebook contains the scraper for product page only. The links for these products have been previously scraped using a different crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "#import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import pandas.io.sql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brand(bb_pad):\n",
    "    \"\"\"get the part manufacturer/brand of the product\"\"\"\n",
    "    return(bb_pad.find_all(\"span\")[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(bb_pad):\n",
    "    \"\"\"get the title of the product listing\"\"\"\n",
    "    return bb_pad.find_all(\"div\", class_ = \"bb_title\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(bb_pad):\n",
    "    \"\"\"get features and notes from the description and separate the feature header from feature description\"\"\"\n",
    "    \n",
    "    notes = (bb_pad.find_all(\"div\", class_ = \"bb_notes\"))\n",
    "    features = {}\n",
    "    for i in notes:\n",
    "        bb_bb = (i.find_all(\"span\", class_ = \"bb_bb\"))\n",
    "        bb_b = i.find_all(\"div\")\n",
    "\n",
    "    for i in range(len(bb_bb)):\n",
    "        features.update({bb_bb[i].text.strip(\":\").replace(\" \", \"_\") : bb_b[i].text.replace(bb_bb[i].text, '')})\n",
    "        \n",
    "    return json.dumps(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(bb_pad):\n",
    "    \"\"\"get images of the product and replace 'thumb' with 'full' for full size image\"\"\"\n",
    "    product_images = []\n",
    "\n",
    "    image = bb_pad.find_all(\"div\", {\"class\" : \"bb_image\"})\n",
    "\n",
    "    for i in image:\n",
    "        a = i.find_all(\"img\")\n",
    "        for a2 in a:\n",
    "            if a2['src'].endswith(\"jpg\"):\n",
    "                product_images.append(a2['src'].replace(\"thumb\", \"full\"))\n",
    "                \n",
    "    return(product_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(bb_b):\n",
    "    \n",
    "    \"\"\"get table with car fitment information\"\"\"\n",
    "    \n",
    "    table = bb_b.find_all(\"div\", {\"class\" : \"bb_pf\"})\n",
    "    #headers = {}\n",
    "    for i in table:\n",
    "        rows = (i.find_all(\"tr\"))\n",
    "        headers = i.find_all(\"th\")\n",
    "        \n",
    "    ret_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(2, len(rows)):\n",
    "        attr = rows[i].find_all(\"td\")\n",
    "        obj = {}\n",
    "        for n in range(len(headers)):\n",
    "            obj[headers[n].get_text()] = attr[n].get_text()\n",
    "        ret_df = ret_df.append(pd.DataFrame(data = [obj]))\n",
    "    \n",
    "    ret_df.drop_duplicates(inplace = True)\n",
    "    ret_df.reset_index(inplace = True, drop = True)\n",
    "    result = ret_df.to_json(orient = \"index\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_data(soup, link):\n",
    "    \"\"\"use all above functions to scrape parts data along with their fitment\"\"\"\n",
    "    \n",
    "    all_parts_df = pd.DataFrame()\n",
    "    \n",
    "    prices = [item.text for item in soup.find_all(\"span\", {\"itemprop\" : \"price\"})]\n",
    "    category = [cat.text for cat in soup.find_all(\"div\", {\"id\" : \"bcrumbs\"})][0]\n",
    "    for i in soup.find_all(\"td\", class_ = \"bb_pad\"):\n",
    "        parts_dict = {}\n",
    "        parts_dict.update({\"brand\" : get_brand(i),\"title\" : get_title(i), \"price\" : [item.text for item in i.find_all(\"span\", {\"itemprop\" : \"price\"})][0]})\n",
    "        parts_dict.update({\"features\" : get_features(i)})\n",
    "        parts_dict.update({\"image\" : get_images(i), \"category\" : category})\n",
    "        parts_dict.update({\"fitment\" : get_table(i), \"link\" : link})\n",
    "        all_parts_df = all_parts_df.append(pd.DataFrame(data = [parts_dict]))\n",
    "    return all_parts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file containing the scraped links to all product pages\n",
    "\n",
    "file = pd.read_csv(\"part_links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://{username}:{password}@{host}:{port}/{database}')\n",
    "for i in range(len(file)):\n",
    "    try:\n",
    "    \n",
    "        response = requests.get(file['part_links'][i])\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        all_parts_data_df = parts_data(soup, file['part_links'][i])\n",
    "        all_parts_data_df.to_sql('{inser table name}', con=engine, index=False, if_exists='append', schema = '{insert schema name}') #insert into sql database\n",
    "        \n",
    "        print(i, len(file))\n",
    "\n",
    "    except:\n",
    "        print(file['part_links'][i]) # print link of failed page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
